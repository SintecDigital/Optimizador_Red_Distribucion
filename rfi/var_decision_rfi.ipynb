{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea central para tener los valores de T1, T2, y exportación es crear tablas con acciones homónimas a las tablas máster. Se debe elegir cuánto es envíado y en qué tipo de camiones y listo. Para exportación, usaremos la info enviada por Salvador. \n",
    "\n",
    "Retos:\n",
    "1. Cómo definir cuántos camiones y de qué tipo para suplir demandas por ciudad por mes. Sería bueno una función interactiva\n",
    "2. Asegurar que sí cruce la información \n",
    "\n",
    "\n",
    "Lo fundamental aquí es que se logre convertir todas las tablas de T1, T2, y EXP al formato de MES, FAMILIA, ID_ORIGEN, ID_DESTINO, CANTIDAD. El caso de exportación es especial, porque es una red muy simple, por lo que podemos hacer dos cosas:\n",
    "1. Calcular el costo total por envío a través del ID_VIAJE\n",
    "2. Crear un factor de eficiencia, para poder simular esa ineficiencia de envíos en un valor unificado medio.\n",
    "\n",
    "\n",
    "Con eso en cuenta, resolveremos esto así:\n",
    "1. Editar las bases de datos para que las columnas tengan los nombres que usamos en los masters\n",
    "2. Editar las bases de datos para que CGNA sea reemplazado por CGNA_PLANT para nacional, y CGNA_PORT para exportación\n",
    "3. Borrar filas que no tengan origen, destino, o sku. \n",
    "4. Crear columnas de año y mes por separado. Filtrar solo 2019\n",
    "5. Unir el diccionario de familias a los productos en todas las tablas\n",
    "6. Pasar a positivo las cantidades y dividirlas por 1000 para que estén en toneladas. Agrupar datos. Al hacer un join con un tarifario de valores únicos, se nos duplicarán los valores en cantidad. Eso es bueno porque podemos pegar una nueva columna al tarifario que sea tasa de utilizacion, inspirado en los valores hechos antes para el baseline. Luego es cuestión de multiplicar la cantidad por ese valor, y tendremos nuestra cantidad de material por tipo de camión. Finalmente, es cuestión de dividir ese valor por la capacidad de camión. Ahí tendremos nuestra variable de decisión\n",
    "7. Unir tablas con el tarifario. Verificar cuáles no cruzan y por qué\n",
    "8. Usar la estructura previamente usada en el baseline de porcentajes de uso por tipo de camión para el caso nacional. Para el caso de exportación, usar el más grande nada más\n",
    "8. Edit: podríamos usar un factor de subutilización de camiones de 34 toneladas en Nacional. Sería fácil de implementar, pero no sé si crearía problemas más adelante. Por el momento hagamos la parte difícil.\n",
    "9. Validar con el mismo procedimiento de limpieza_master para ver si hay errores de tipografía y corregirlos.\n",
    "10. Cruzar con la tabla de variables_decision para obtener el costo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sku'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-48ec0f739b7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Mapear diccionario de SKUs a familias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mt1_rfi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt1_rfi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_sku_fam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sku'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mt1_rfi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt1_rfi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'familia'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'NAN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   7295\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7296\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7297\u001b[1;33m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7298\u001b[0m         )\n\u001b[0;32m   7299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    994\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1690\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sku'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "t1_rfi = pd.read_csv('rfi/T1_RFI.csv')\n",
    "t2_rfi = pd.read_csv('rfi/T2_RFI.csv')\n",
    "dict_sku_fam = pd.read_excel('rfi/diccionario_sku_familia.xlsx')\n",
    "tarifario = pd.read_csv('input_final/master_tarifario.csv')\n",
    "## Pendiente exportacion\n",
    "\n",
    "\n",
    "#### Costos T1 ####\n",
    "\n",
    "# Mapear diccionario de SKUs a familias\n",
    "t1_rfi = t1_rfi.merge(dict_sku_fam, how='left', on='sku')\n",
    "t1_rfi = t1_rfi.fillna(values={'familia': 'NAN'})\n",
    "\n",
    "# Pasar a positivo las cantidades, seleccionar solo 2019\n",
    "t1_rfi['cantidad'] *= -1\n",
    "t1_rfi = t1_rfi.astype({'fecha_viaje': 'datetime64'})\n",
    "t1_rfi = t1_rfi[t1_rfi['fecha_viaje'].dt.year == 2019]\n",
    "\n",
    "# Crear Col de mes\n",
    "t1_rfi['mes'] = t1_rfi['fecha_viaje'].dt.month\n",
    "\n",
    "# Reemplazar CGNA por CGNA_PLANT\n",
    "t1_rfi['id_ciudad_origen'] = t1_rfi['id_ciudad_origen'].str.replace('CGNA', 'CGNA_PLANT')\n",
    "\n",
    "# Groupby\n",
    "\n",
    "\n",
    "# Crear columna transporte. Esto lo haremos a partir de una división por los tipos de transportes disponibles.\n",
    "# Para ello, haremos un merge con el tarifario. \n",
    "t1_rfi = t1_rfi.merge(tarifario, how='inner', on=['id_ciudad_origen', 'id_ciudad_destino'])\n",
    "t1_rfi = t1_rfi.loc[:, ['mes', 'familia', 'capacidad', 'id_ciudad_origen', 'id_ciudad_destino', 'costo']]\n",
    "\n",
    "# Ahora solo falta agregar los valores a esto. Hay que asignarlas de acuerdo a los tipos de capacidad disponibles. Dado que en exportación hay tantas opciones,\n",
    "# va a ser viable usar la de mayor capacidad y combinarla con la de menor capacidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Función para alistar los datos\n",
    "\n",
    "def exp_clean(data):\n",
    "    \n",
    "    data['id_ciudad_origen'] = 'CGNA_PORT'\n",
    "    \n",
    "    \n",
    "\n",
    "def remover_tildes_espacios(series):\n",
    "    \"\"\"\n",
    "    Remueve tildes y espacios al inicio y al final de los strings en una pd.Series\n",
    "    :param series: pd.Series\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    series = series.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    series = series.str.strip()\n",
    "    return series\n",
    "\n",
    "\n",
    "def t_decision(data, fam_dict, tarifario, year=2019, tipo: str):\n",
    "    \"\"\"\n",
    "    Limpia datos de T1 y T2. Tambien calcula las variables de decisión\n",
    "    \n",
    "    :params data: datos de demanda limpios, con estructura de master demanda\n",
    "    :params fam_dict: Diccionario de familias y SKUs\n",
    "    :params tarifario: Tarifario de transporte\n",
    "    :params tipo: indicar si es T1 o T2  # TODO: vale la pena unir t1 y t2 y poner una columna que indique cuál es.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conteo inicial de demanda para saber cuánto se descartó\n",
    "    demanda_inicial = data['cantidad'].sum() / 1000\n",
    "    \n",
    "    # Eliminar filas con NaN en origen, destino, sku\n",
    "    data = data.dropna(subset=['id_ciudad_origen', 'id_ciudad_destino', 'sku'])\n",
    "    \n",
    "    # Configurar columna fecha a formato fecha, crear col de año y mes. Filtrar año en cuestión\n",
    "    data = data.astype({'fecha_viaje': 'datetime64'})\n",
    "    data['año'] = data['fecha'].dt.year\n",
    "    data['mes'] = data['fecha'].dt.month\n",
    "    data = data.loc[data['año'] == year]\n",
    "    \n",
    "    if tipo == 't2':\n",
    "        data = data.loc[data['nacional_exportacion'] == 'Nacional']\n",
    "        data['cantidad'] *= (1/1000)\n",
    "    elif tipo == 't1':\n",
    "        data['cantidad'] *= (-1/1000)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Seleccionar solo columnas relevantes\n",
    "    data = data.loc[:, ['mes', 'id_ciudad_origen', 'id_ciudad_destino', 'sku', 'cantidad']]\n",
    "    \n",
    "    # Unir familias a data de demanda\n",
    "    data = data.merge(fam_dict, how='left', on='sku')\n",
    "    \n",
    "    data_omitida = data.loc[data['familia'].isna()]\n",
    "    data = data.loc[~data['familia'].isna()]\n",
    "    \n",
    "    # Limpiar textos\n",
    "    for col in ['id_ciudad_origen', 'id_ciudad_destino', 'familia']:\n",
    "        data[i] = data[i].str.upper()\n",
    "        data[i] = remover_tildes_espacios(data[i])\n",
    "        \n",
    "    \n",
    "    # Agrupar datos\n",
    "    data = data.groupby(['mes', 'familia', 'id_ciudad_origen', 'id_ciudad_destino', 'cantidad']).sum().reset_index()\n",
    "\n",
    "    ## CALCULO variables decision ##\n",
    "    # Lograr poner en tarifario diccionario de porcentajes de uso. La idea es que en el tarifario podamos poner estos pesos. Para ello,\n",
    "    # primero hay que filtrar las tarifas que son nacionales y omitir aquellas que tienen un 999. Para ello haremos una variación en\n",
    "    # el diccionario de pesos usado en el tarifario. Habían cinco casos, donde dos de ellos (que haya solo dos tarifas aunque de diferentes\n",
    "    # pesos), son iguales en la cantidad de tarifas disponibles. El cambio es el siguiente. Si hay solo 1 transporte, va 100%; si hay dos, \n",
    "    # va el 95% en el más alto y 5%; si hay tres, va 90%, 5%, 5%; y si hay cuatro, van 95%, 3%, 1%, 1%\n",
    "    tarifario = tarifario.loc[(tarifario['id_ciudad_origen'].isin(['MB_PLANT', 'CGNA_PLANT', 'ABOD', 'BBOD', 'CBOD', 'BUEN_PORT'])) &\n",
    "                             (tarifario['capacidad'] != 999)]\n",
    "    pesos_camion_dict = {1: [1], 2:[0.95, 0.05], 3: [0.9, 0.05, 0.05], 4: [0.95, 0.03, 0.01, 0.01]}\n",
    "    \n",
    "    # Hacer un groupby provisional para saber cuántos transportes hay por rutas. Luego lo unimos con tarifario organizado por alfabeto\n",
    "    tarifario_group = tarifario.groupby(['id_ciudad_origen', 'id_ciudad_destino'])['capacidad'].count()\n",
    "    tarifario_group = tarifario_group.rename(columns={'capacidad': 'conteo'})\n",
    "    tarifario = tarifario.sort_values(['id_ciudad_origen', 'id_ciudad_destino', 'capacidad'], ascending=[True, True, False])\n",
    "    \n",
    "    # Unir conteo con tarifario. Teniendo conteo, agregar valores de pesos de camiones en tasa_util\n",
    "    tarifario = tarifario.merge(tarifario_group, on=['id_ciudad_origen', 'id_ciudad_destino'])\n",
    "    del(tarifario_group)\n",
    "    tarifario['tasa_util'] = 0\n",
    "    for i in tarifario['conteo'].unique():\n",
    "        tarifario.loc[tarifario['conteo'] == i, 'tasa_util'] = pesos_camion_dict[i] * tarifario.loc[tarifario['conteo'] == i, 'conteo'].shape[0]\n",
    "    \n",
    "    # Cruzar demanda con tarifario, guardar omitidos\n",
    "    data = data.merge(tarifario, how='left', on=['id_ciudad_origen', 'id_ciudad_destino'])\n",
    "    data_omitida = pd.concat(data_omitida, data.loc[data['capacidad'].isna()], ignore_index=True)\n",
    "    data = data.loc[~data['capacidad'].isna()]\n",
    "        \n",
    "    # Calcular variable de decisión, que equivale a (tasa_util * cantidad) / capacidad\n",
    "    data['valor_decision'] = (data['tasa_util'] * data['cantidad']) / data['capacidad']\n",
    "    \n",
    "    # Filtrar columnas relevantes\n",
    "    data = data.loc[:, ['mes', 'familia', 'capacidad', 'id_ciudad_origen', 'id_ciudad_destino', 'costo', 'valor_decision']]\n",
    "    \n",
    "    return data, data_omitida\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1, 2, 3, 4], [3, 4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['al'] = [33] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarifario.groupby(['id_ciudad_origen', 'id_ciudad_destino'])['capacidad'].count().reset_index()\n",
    "tarifario = tarifario.sort_values(['id_ciudad_origen', 'id_ciudad_destino', 'capacidad'], ascending=[True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id_ciudad_origen</th>\n",
       "      <th>id_ciudad_destino</th>\n",
       "      <th>capacidad_x</th>\n",
       "      <th>costo</th>\n",
       "      <th>capacidad_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABOD</td>\n",
       "      <td>ABOD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64488.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ABOD</td>\n",
       "      <td>APARTADO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64488.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ABOD</td>\n",
       "      <td>ARMENIA</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3600000.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ABOD</td>\n",
       "      <td>ARMENIA</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>ABOD</td>\n",
       "      <td>ARMENIA</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1270000.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>984</td>\n",
       "      <td>MB_PLANT</td>\n",
       "      <td>PEREIRA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176380.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>985</td>\n",
       "      <td>MB_PLANT</td>\n",
       "      <td>PGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>986</td>\n",
       "      <td>MB_PLANT</td>\n",
       "      <td>RIONEGRO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153600.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>987</td>\n",
       "      <td>MB_PLANT</td>\n",
       "      <td>TOCANCIPA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>988</td>\n",
       "      <td>MB_PLANT</td>\n",
       "      <td>YOPAL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>166400.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 id_ciudad_origen id_ciudad_destino  capacidad_x      costo  \\\n",
       "0             0             ABOD              ABOD          1.0    64488.0   \n",
       "1             1             ABOD          APARTADO          1.0    64488.0   \n",
       "2             5             ABOD           ARMENIA         34.0  3600000.0   \n",
       "3             4             ABOD           ARMENIA         17.0  2000000.0   \n",
       "4             3             ABOD           ARMENIA          8.5  1270000.0   \n",
       "..          ...              ...               ...          ...        ...   \n",
       "979         984         MB_PLANT           PEREIRA          1.0   176380.0   \n",
       "980         985         MB_PLANT               PGI          1.0   195000.0   \n",
       "981         986         MB_PLANT          RIONEGRO          1.0   153600.0   \n",
       "982         987         MB_PLANT         TOCANCIPA          1.0   160000.0   \n",
       "983         988         MB_PLANT             YOPAL          1.0   166400.0   \n",
       "\n",
       "     capacidad_y  \n",
       "0              1  \n",
       "1              1  \n",
       "2              4  \n",
       "3              4  \n",
       "4              4  \n",
       "..           ...  \n",
       "979            1  \n",
       "980            1  \n",
       "981            1  \n",
       "982            1  \n",
       "983            1  \n",
       "\n",
       "[984 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarifario.merge(tar, on=['id_ciudad_origen', 'id_ciudad_destino'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -999.0\n",
       "1     -999.0\n",
       "14    -999.0\n",
       "27    -999.0\n",
       "40    -999.0\n",
       "       ...  \n",
       "979   -999.0\n",
       "980   -999.0\n",
       "981   -999.0\n",
       "982   -999.0\n",
       "983   -999.0\n",
       "Name: costo, Length: 74, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarifario_del.loc[tarifario_del['capacidad'] == 1, 'costo'] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarifario_del['a'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarifario_del.capacidad.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_coef(items_df: pd.DataFrame, actividades_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Contruye matriz de coeficientes con base a las actividades (columnas) e ítems (filas) ingresadas.\n",
    "\n",
    "    Retorna un np.array de coeficientes, siendo los indices `items_df`, y las columnas `actividades_df`.\n",
    "\n",
    "    :param items_df:\n",
    "    :param actividades_df:\n",
    "    :return coef_mat:\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear matriz de coeficientes con ceros para llenar\n",
    "    coef_mat = np.zeros((items_df.shape[0], actividades_df.shape[0]))\n",
    "\n",
    "    for idx, item in items_df.iterrows():\n",
    "        for idy, actividad in actividades_df.iterrows():\n",
    "\n",
    "            # 1. Condicion de entrada de flujo\n",
    "            if (item['tiempo'] == actividad['tiempo'] and\n",
    "                    item['producto'] == actividad['producto'] and\n",
    "                    item['nodo'] == actividad['origen']):\n",
    "                coef_mat[idx, idy] = actividad['transporte']\n",
    "\n",
    "            # 2. Condicion de salida de flujo\n",
    "            elif (item['tiempo'] == actividad['tiempo'] and\n",
    "                  item['producto'] == actividad['producto'] and\n",
    "                  item['nodo'] == actividad['destino']):\n",
    "                coef_mat[idx, idy] = -actividad['transporte']\n",
    "\n",
    "            # 3. Condicion de entrada de input a almacenamiento\n",
    "            elif (item['tiempo'] == actividad['tiempo'] and\n",
    "                    item['producto'] == actividad['producto'] and\n",
    "                    'ALMACENAMIENTO' in actividad['origen'] and\n",
    "                    item['nodo'] == actividad['origen'].replace('_ALMACENAMIENTO', '')):\n",
    "                coef_mat[idx, idy] = 1\n",
    "\n",
    "            # 4. Condicion de output almacenamiento\n",
    "            elif ((item['tiempo'] - 1) == actividad['tiempo'] and\n",
    "                  item['producto'] == actividad['producto'] and\n",
    "                  'ALMACENAMIENTO' in actividad['destino'] and\n",
    "                  item['nodo'] == actividad['destino'].replace('_ALMACENAMIENTO', '')):\n",
    "                coef_mat[idx, idy] = -1\n",
    "\n",
    "            # 5. Condicion de máximo almacenamiento (capacidad estática)\n",
    "            elif (item['tiempo'] == actividad['tiempo'] and\n",
    "                  'ALMACENAMIENTO' in actividad['origen'] and\n",
    "                  item['nodo'] == actividad['origen'].replace('_ALMACENAMIENTO', '') and\n",
    "                  item['tipo'] == 'capacidad_est'):\n",
    "                coef_mat[idx, idy] = 1\n",
    "\n",
    "            # 6. Condicion de máximo flujo (capacidad dinámica)\n",
    "            elif (item['tiempo'] == actividad['tiempo'] and\n",
    "                  item['tipo'] == 'capacidad_din' and\n",
    "                  item['nodo'] == actividad['destino']):\n",
    "                coef_mat[idx, idy] = actividad['transporte']\n",
    "            else:\n",
    "                continue\n",
    "    return coef_mat\n",
    "\n",
    "def matriz_coef_v2(items_df: pd.DataFrame, actividades_df: pd.DataFrame):\n",
    "    \n",
    "    coef_mat = np.zeros((items_df.shape[0], actividades_df.shape[0]))\n",
    "    \n",
    "    # Crar columnas de indice de items y actividades\n",
    "    actividades_df['idy'] =actividades.index\n",
    "    items_df['idx'] = items_df.index\n",
    "    \n",
    "    ## Al ser seis grupos de condiciones, serían 6 JOIN. CONDICIONES:\n",
    "    \n",
    "    # ENTRADA DE FLUJO. al ser INNER, no habrá valores nulos. Aunque algo más eficiente es ir acortando el número de filas\n",
    "    # EDIT: no es tan fácil reusar lo que no cruzó, porque los cruces en las condiciones no son de las mismas columnas\n",
    "    cond1 = pd.merge(items_df, actividades_df, left_on=['tiempo', 'producto', 'nodo'],\n",
    "                     right_on=['tiempo', 'producto', 'origen'], how='inner')\n",
    "    cond1['valor_mat'] = cond1['transporte'].copy()\n",
    "    \n",
    "    # SALIDA DE FLUJO\n",
    "    cond2 = pd.merge(items_df, actividades_df, left_on=['tiempo', 'producto', 'nodo'],\n",
    "                     right_on=['tiempo', 'producto', 'destino'], how='inner')\n",
    "    cond2['valor_mat'] = cond2['transporte'].copy()\n",
    "    \n",
    "    # ENTRADA INPUT A ALMACENAMIENTO\n",
    "    cond3_items = item_df.copy()\n",
    "    cond3_items['nodo'] = cond3_items['nodo'] + '_ALMACENAMIENTO'\n",
    "    cond3 = pd.merge(cond3_items, actividades_df, left_on=['tiempo', 'producto', 'nodo'],\n",
    "                     right_on=['tiempo', 'producto', 'origen'], how='inner')\n",
    "    cond3['valor_mat'] = 1\n",
    "    del cond3_items\n",
    "    \n",
    "    # SALIDA OUTPUT ALMACENAMIENTO\n",
    "    cond4_items = item_df.copy()\n",
    "    cond4_items['tiempo'] -= 1\n",
    "    cond4_items['nodo'] = cond4_items['nodo'] + '_ALMACENAMIENTO'\n",
    "    cond4 = pd.merge(cond4_items, actividades_df, left_on=['tiempo', 'producto', 'nodo'],\n",
    "                 right_on=['tiempo', 'producto', 'destino'], how='inner')\n",
    "    cond4['valor_mat'] = -1\n",
    "    del cond4_items\n",
    "    \n",
    "    # MAXIMO ALMACENAMIENTO (CAP ESTATICA)\n",
    "    cond5_items = items_df.loc[items_df['tipo'] == 'capacidad_est']\n",
    "    cond5_items['nodo'] = cond5_items['nodo'] + '_ALMACENAMIENTO'\n",
    "    cond5 = pd.merge(cond5_items, actividades_df, left_on=['tiempo', 'nodo'], right_on=['tiempo', 'origen'], how='inner')\n",
    "    cond5['valor_mat'] = 1\n",
    "    del cond5_items\n",
    "    \n",
    "    # MAXIMO FLUJO (CAP DINAMICA)\n",
    "    cond6_items = items_df.loc[items_df['tipo'] == 'capacidad_din']\n",
    "    cond6 = pd.merge(cond6_items, actividades_df, left_on=['tiempo', 'nodo'], right_on=['tiempo', 'destino'], how='inner')\n",
    "    cond6['valor_mat'] = cond6['transporte'].copy()\n",
    "    del cond6_items\n",
    "    \n",
    "    condiciones = pd.concat([cond1, cond2, cond3, cond4, cond5, cond6], ignore_index=True)\n",
    "    \n",
    "    # Crear matriz de coeficiente a partir de tabla de condiciones\n",
    "    for index, condicion in condiciones.iterrows():\n",
    "        coef_mat[condicion['idx'], condicion['idy']] = condicion['valor_mat']\n",
    "        \n",
    "    return coef_mat\n",
    "    \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
